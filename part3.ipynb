{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading treebank data...\n",
      "Train set size:  51681\n",
      "Dev set size:  7863\n",
      "Test set size:  9046\n",
      "Viterbi Decoding Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   0.00      0.00      0.00         0\n",
      "           #       0.00      0.00      0.00        22\n",
      "           $       0.00      0.00      0.00      1138\n",
      "          ''       1.00      0.23      0.38      1423\n",
      "           (       1.00      0.00      0.01       249\n",
      "           )       1.00      0.22      0.36       252\n",
      "           ,       1.00      0.00      0.00      9056\n",
      "           .       1.00      1.00      1.00      7035\n",
      "           :       1.00      0.04      0.08       983\n",
      "     <START>       0.00      0.00      0.00         0\n",
      "          CC       0.00      0.00      0.00      4289\n",
      "          CD       0.99      0.03      0.05      6023\n",
      "          DT       0.86      0.01      0.02     14946\n",
      "          EX       1.00      0.02      0.03       174\n",
      "          FW       0.50      0.03      0.05        38\n",
      "          IN       1.00      0.00      0.00     18147\n",
      "          JJ       0.88      0.00      0.00     10704\n",
      "         JJR       0.00      0.00      0.00       581\n",
      "     JJR|RBR       0.00      0.00      0.00         4\n",
      "         JJS       0.50      0.01      0.02       374\n",
      "       JJ|IN       0.00      0.00      0.00         1\n",
      "          LS       0.00      0.00      0.00        15\n",
      "          MD       0.00      0.00      0.00      1674\n",
      "          NN       0.97      0.02      0.05     23468\n",
      "         NNP       0.94      0.01      0.01     17236\n",
      "        NNPS       0.00      0.00      0.00       239\n",
      "         NNS       0.99      0.04      0.07     10697\n",
      "         PDT       0.00      0.00      0.00        66\n",
      "         POS       0.33      0.00      0.00      1638\n",
      "         PRP       1.00      0.03      0.07      2930\n",
      "        PRP$       0.00      0.00      0.00      1433\n",
      "          RB       0.79      0.00      0.01      5853\n",
      "         RBR       0.20      0.01      0.01       382\n",
      "     RBR|JJR       0.00      0.00      0.00         2\n",
      "         RBS       0.00      0.00      0.00        87\n",
      "       RB|JJ       0.00      0.00      0.00         2\n",
      "          RP       0.00      0.00      0.00       356\n",
      "         SYM       0.00      0.00      0.00        11\n",
      "          TO       0.00      0.00      0.00      3899\n",
      "          UH       0.00      0.00      0.00        20\n",
      "          VB       0.33      0.00      0.00      4766\n",
      "         VBD       0.50      0.00      0.00      5869\n",
      "         VBG       1.00      0.00      0.00      2592\n",
      "      VBG|NN       0.00      0.00      0.00         1\n",
      "         VBN       1.00      0.00      0.00      3580\n",
      "      VBN|JJ       0.00      0.00      0.00         2\n",
      "         VBP       0.00      0.00      0.00      2209\n",
      "         VBZ       1.00      0.00      0.00      3608\n",
      "         WDT       0.97      0.09      0.17       773\n",
      "          WP       0.99      0.17      0.28       397\n",
      "         WP$       1.00      0.15      0.26        47\n",
      "         WRB       0.00      0.00      0.00       425\n",
      "          ``       0.00      0.00      0.00      1422\n",
      "\n",
      "    accuracy                           0.05    171138\n",
      "   macro avg       0.45      0.04      0.06    171138\n",
      "weighted avg       0.81      0.05      0.06    171138\n",
      "\n",
      "Baseline Tagger Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           #       1.00      1.00      1.00        22\n",
      "           $       1.00      1.00      1.00      1138\n",
      "          ''       1.00      0.99      1.00      1423\n",
      "           (       1.00      1.00      1.00       249\n",
      "           )       1.00      1.00      1.00       252\n",
      "           ,       1.00      1.00      1.00      9056\n",
      "           .       1.00      1.00      1.00      7035\n",
      "           :       1.00      1.00      1.00       983\n",
      "          CC       1.00      1.00      1.00      4289\n",
      "          CD       0.99      0.90      0.94      6023\n",
      "          DT       0.99      0.99      0.99     14946\n",
      "          EX       0.89      1.00      0.94       174\n",
      "          FW       0.38      0.24      0.29        38\n",
      "          IN       0.94      0.98      0.96     18147\n",
      "          JJ       0.88      0.86      0.87     10704\n",
      "         JJR       0.66      0.95      0.78       581\n",
      "     JJR|RBR       0.00      0.00      0.00         4\n",
      "         JJS       0.79      0.95      0.86       374\n",
      "       JJ|IN       0.00      0.00      0.00         1\n",
      "          LS       0.00      0.00      0.00        15\n",
      "          MD       0.99      1.00      0.99      1674\n",
      "          NN       0.80      0.94      0.86     23468\n",
      "         NNP       0.97      0.86      0.91     17236\n",
      "        NNPS       0.25      0.45      0.32       239\n",
      "         NNS       0.97      0.94      0.96     10697\n",
      "     NNS|VBZ       0.00      0.00      0.00         0\n",
      "         PDT       0.00      0.00      0.00        66\n",
      "         POS       0.87      1.00      0.93      1638\n",
      "         PRP       1.00      0.99      1.00      2930\n",
      "        PRP$       0.99      1.00      0.99      1433\n",
      "          RB       0.90      0.85      0.88      5853\n",
      "         RBR       0.83      0.24      0.37       382\n",
      "     RBR|JJR       0.00      0.00      0.00         2\n",
      "         RBS       0.40      0.02      0.04        87\n",
      "       RB|JJ       0.00      0.00      0.00         2\n",
      "          RP       0.34      0.12      0.17       356\n",
      "         SYM       0.80      0.73      0.76        11\n",
      "          TO       1.00      1.00      1.00      3899\n",
      "          UH       1.00      0.50      0.67        20\n",
      "          VB       0.79      0.70      0.74      4766\n",
      "         VBD       0.86      0.84      0.85      5869\n",
      "         VBG       0.90      0.85      0.87      2592\n",
      "      VBG|NN       0.00      0.00      0.00         1\n",
      "         VBN       0.73      0.72      0.72      3580\n",
      "      VBN|JJ       0.00      0.00      0.00         2\n",
      "         VBP       0.75      0.71      0.73      2209\n",
      "         VBZ       0.95      0.87      0.91      3608\n",
      "         WDT       1.00      0.54      0.70       773\n",
      "          WP       0.99      1.00      0.99       397\n",
      "         WP$       1.00      1.00      1.00        47\n",
      "         WRB       1.00      0.99      1.00       425\n",
      "          ``       1.00      1.00      1.00      1422\n",
      "\n",
      "    accuracy                           0.92    171138\n",
      "   macro avg       0.72      0.69      0.69    171138\n",
      "weighted avg       0.92      0.92      0.92    171138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn import metrics\n",
    "\n",
    "def evaluate(test_sentences, tagged_test_sentences):\n",
    "    gold = [str(tag) for sentence in test_sentences for token, tag in sentence]\n",
    "    pred = [str(tag) for sentence in tagged_test_sentences for token, tag in sentence]\n",
    "    print(metrics.classification_report(gold, pred))\n",
    "\n",
    "def get_token_tag_tuples(sent):\n",
    "    return [nltk.tag.str2tuple(t) for t in sent.split()]\n",
    "\n",
    "def get_tagged_sentences(text):\n",
    "    sentences = []\n",
    "    blocks = text.split(\"======================================\")\n",
    "    for block in blocks:\n",
    "        sents = block.split(\"\\n\\n\")\n",
    "        for sent in sents:\n",
    "            sent = sent.replace(\"\\n\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "            if sent != \"\":\n",
    "                sentences.append(sent)\n",
    "    return sentences\n",
    "\n",
    "def load_treebank_splits(datadir):\n",
    "    train = []\n",
    "    dev = []\n",
    "    test = []\n",
    "    print(\"Loading treebank data...\")\n",
    "    for subdir, dirs, files in os.walk(datadir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".pos\"):\n",
    "                filepath = subdir + os.sep + filename\n",
    "                with open(filepath, \"r\") as fh:\n",
    "                    text = fh.read()\n",
    "                    if int(subdir.split(os.sep)[-1]) in range(0, 19):\n",
    "                        train += get_tagged_sentences(text)\n",
    "                    if int(subdir.split(os.sep)[-1]) in range(19, 22):\n",
    "                        dev += get_tagged_sentences(text)\n",
    "                    if int(subdir.split(os.sep)[-1]) in range(22, 25):\n",
    "                        test += get_tagged_sentences(text)\n",
    "    print(\"Train set size: \", len(train))\n",
    "    print(\"Dev set size: \", len(dev))\n",
    "    print(\"Test set size: \", len(test))\n",
    "    return train, dev, test\n",
    "\n",
    "def compute_transition_emission_tables(sentences, alpha=1):\n",
    "    transition_counts = defaultdict(Counter)\n",
    "    emission_counts = defaultdict(Counter)\n",
    "    tag_counts = Counter()\n",
    "\n",
    "    for sent in sentences:\n",
    "        tokens_tags = [('<START>', '<START>')] + get_token_tag_tuples(sent) + [('<STOP>', '<STOP>')]\n",
    "        for i in range(len(tokens_tags) - 1):\n",
    "            prev_tag, curr_tag = tokens_tags[i][1], tokens_tags[i + 1][1]\n",
    "            token, tag = tokens_tags[i + 1]\n",
    "            transition_counts[prev_tag][curr_tag] += 1\n",
    "            if tag != '<START>' and tag != '<STOP>':  # <START> and <STOP> don't emit tokens\n",
    "                emission_counts[tag][token] += 1\n",
    "            tag_counts[tag] += 1\n",
    "\n",
    "    # Add <START> and <STOP> explicitly\n",
    "    tag_counts['<START>'] += 1\n",
    "    tag_counts['<STOP>'] += 1\n",
    "\n",
    "    # Convert counts to log probabilities\n",
    "    transition_probs = {tag: {next_tag: math.log((count + alpha) / (sum(transition_counts[tag].values()) + alpha * len(tag_counts)))\n",
    "                              for next_tag, count in next_tags.items()}\n",
    "                        for tag, next_tags in transition_counts.items()}\n",
    "\n",
    "    # Ensure <START> and <STOP> have no emissions\n",
    "    emission_probs = {tag: {word: math.log((count + alpha) / (tag_counts[tag] + alpha * len(emission_counts[tag])))\n",
    "                            for word, count in words.items()}\n",
    "                      for tag, words in emission_counts.items()}\n",
    "    emission_probs['<START>'] = {}\n",
    "    emission_probs['<STOP>'] = {}\n",
    "\n",
    "    return transition_probs, emission_probs\n",
    "\n",
    "\n",
    "def viterbi(sentence, transition_probs, emission_probs, tags):\n",
    "    n = len(sentence)\n",
    "    viterbi_table = defaultdict(lambda: defaultdict(float))\n",
    "    backpointer = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "    # Initialization\n",
    "    for tag in tags:\n",
    "        if tag != '<START>':  # <START> doesn't emit tokens\n",
    "            viterbi_table[0][tag] = transition_probs['<START>'].get(tag, float('-inf')) + emission_probs[tag].get(sentence[0], float('-inf'))\n",
    "            backpointer[0][tag] = '<START>'\n",
    "\n",
    "    # Recursion\n",
    "    for t in range(1, n):\n",
    "        for tag in tags:\n",
    "            if tag != '<START>' and tag != '<STOP>':  # Skip invalid transitions\n",
    "                max_prob, best_prev_tag = max(\n",
    "                    (viterbi_table[t-1][prev_tag] + transition_probs[prev_tag].get(tag, float('-inf')) + emission_probs[tag].get(sentence[t], float('-inf')),\n",
    "                     prev_tag) for prev_tag in tags if prev_tag != '<STOP>')\n",
    "                viterbi_table[t][tag] = max_prob\n",
    "                backpointer[t][tag] = best_prev_tag\n",
    "\n",
    "    # Termination\n",
    "    max_prob, last_tag = max(\n",
    "        (viterbi_table[n-1][tag] + transition_probs[tag].get('<STOP>', float('-inf')), tag)\n",
    "        for tag in tags if tag != '<START>'\n",
    "    )\n",
    "\n",
    "    # Backtrace\n",
    "    best_path = []\n",
    "    current_tag = last_tag\n",
    "    for t in range(n-1, -1, -1):\n",
    "        best_path.insert(0, current_tag)\n",
    "        current_tag = backpointer[t][current_tag]\n",
    "\n",
    "    return best_path\n",
    "\n",
    "\n",
    "def baseline_tagger(train_sentences, test_sentences):\n",
    "    # Build a most frequent tagger\n",
    "    tag_count = defaultdict(Counter)\n",
    "    for sent in train_sentences:\n",
    "        tokens_tags = get_token_tag_tuples(sent)\n",
    "        for token, tag in tokens_tags:\n",
    "            tag_count[token][tag] += 1\n",
    "\n",
    "    most_frequent_tags = {token: tags.most_common(1)[0][0] for token, tags in tag_count.items()}\n",
    "\n",
    "    tagged_test_sentences = []\n",
    "    for sent in test_sentences:\n",
    "        tokens = [token for token, _ in sent]\n",
    "        predicted_tags = [most_frequent_tags.get(token, 'NN') for token in tokens]\n",
    "        tagged_test_sentences.append(list(zip(tokens, predicted_tags)))\n",
    "\n",
    "    return tagged_test_sentences\n",
    "\n",
    "def main():\n",
    "    # Set path for datadir\n",
    "    datadir = \"data\\penn-treeban3-wsj\\wsj\"\n",
    "\n",
    "    train, dev, test = load_treebank_splits(datadir)\n",
    "\n",
    "    # Compute transition and emission probabilities\n",
    "    transition_probs, emission_probs = compute_transition_emission_tables(train)\n",
    "    tags = list(transition_probs.keys())\n",
    "\n",
    "    # Evaluate on test set with Viterbi decoding\n",
    "    test_sentences = [get_token_tag_tuples(sent) for sent in test]\n",
    "    tagged_test_sentences = []\n",
    "    for sentence in test_sentences:\n",
    "        tokens = [token for token, _ in sentence]\n",
    "        predicted_tags = viterbi(tokens, transition_probs, emission_probs, tags)\n",
    "        tagged_test_sentences.append(list(zip(tokens, predicted_tags)))\n",
    "    \n",
    "    print(\"Viterbi Decoding Evaluation:\")\n",
    "    evaluate(test_sentences, tagged_test_sentences)\n",
    "\n",
    "    # Baseline tagger evaluation\n",
    "    print(\"Baseline Tagger Evaluation:\")\n",
    "    baseline_predictions = baseline_tagger(train, test_sentences)\n",
    "    evaluate(test_sentences, baseline_predictions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
