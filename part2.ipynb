{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_sentences, tagged_test_sentences):\n",
    "    gold = [str(tag) for sentence in test_sentences for token, tag in sentence]\n",
    "    pred = [str(tag) for sentence in tagged_test_sentences for token, tag in sentence]\n",
    "    print(metrics.classification_report(gold, pred))\n",
    "\n",
    "def get_token_tag_tuples(sent):\n",
    "    return [nltk.tag.str2tuple(t) for t in sent.split()]\n",
    "\n",
    "def get_tagged_sentences(text):\n",
    "    sentences = []\n",
    "    blocks = text.split(\"======================================\")\n",
    "    for block in blocks:\n",
    "        sents = block.split(\"\\n\\n\")\n",
    "        for sent in sents:\n",
    "            sent = sent.replace(\"\\n\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "            if sent != \"\":\n",
    "                sentences.append(sent)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_treebank_splits(datadir):\n",
    "    train = []\n",
    "    dev = []\n",
    "    test = []\n",
    "    print(\"Loading treebank data...\")\n",
    "    for subdir, dirs, files in os.walk(datadir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".pos\"):\n",
    "                filepath = subdir + os.sep + filename\n",
    "                with open(filepath, \"r\") as fh:\n",
    "                    text = fh.read()\n",
    "                    if int(subdir.split(os.sep)[-1]) in range(0, 19):\n",
    "                        train += get_tagged_sentences(text)\n",
    "                    if int(subdir.split(os.sep)[-1]) in range(19, 22):\n",
    "                        dev += get_tagged_sentences(text)\n",
    "                    if int(subdir.split(os.sep)[-1]) in range(22, 25):\n",
    "                        test += get_tagged_sentences(text)\n",
    "    print(\"Train set size: \", len(train))\n",
    "    print(\"Dev set size: \", len(dev))\n",
    "    print(\"Test set size: \", len(test))\n",
    "    return train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transition_emission_tables(sentences, alpha=1):\n",
    "    transition_counts = defaultdict(Counter)\n",
    "    emission_counts = defaultdict(Counter)\n",
    "    tag_counts = Counter()\n",
    "\n",
    "    # Include <START> and <STOP> tags during computation\n",
    "    for sent in sentences:\n",
    "        tokens_tags = [('<START>', '<START>')] + get_token_tag_tuples(sent) + [('<STOP>', '<STOP>')]\n",
    "        for i in range(len(tokens_tags) - 1):\n",
    "            prev_tag, curr_tag = tokens_tags[i][1], tokens_tags[i + 1][1]\n",
    "            token, tag = tokens_tags[i + 1]\n",
    "            transition_counts[prev_tag][curr_tag] += 1\n",
    "            emission_counts[tag][token] += 1\n",
    "            tag_counts[tag] += 1\n",
    "\n",
    "    # Add the <START> and <STOP> tags explicitly to tag_counts\n",
    "    tag_counts['<START>'] += 1\n",
    "    tag_counts['<STOP>'] += 1\n",
    "\n",
    "    # Convert counts to probabilities with add-alpha smoothing\n",
    "    transition_probs = {tag: {next_tag: (count + alpha) / (sum(transition_counts[tag].values()) + alpha * len(tag_counts))\n",
    "                              for next_tag, count in next_tags.items()}\n",
    "                        for tag, next_tags in transition_counts.items()}\n",
    "    \n",
    "    # Initialize emission probabilities for <START> and <STOP>\n",
    "    emission_probs = {tag: {word: (count + alpha) / (tag_counts[tag] + alpha * len(emission_counts[tag]))\n",
    "                            for word, count in words.items()}\n",
    "                      for tag, words in emission_counts.items()}\n",
    "    emission_probs['<START>'] = {}\n",
    "    emission_probs['<STOP>'] = {}\n",
    "\n",
    "    return transition_probs, emission_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentence, transition_probs, emission_probs, tags):\n",
    "    n = len(sentence)\n",
    "    viterbi_table = defaultdict(lambda: defaultdict(float))\n",
    "    backpointer = defaultdict(lambda: defaultdict(str))\n",
    "    \n",
    "    # Initialization\n",
    "    for tag in tags:\n",
    "        if tag != '<START>':  # <START> doesn't emit words\n",
    "            viterbi_table[0][tag] = transition_probs['<START>'].get(tag, 0) * emission_probs[tag].get(sentence[0], 0)\n",
    "            backpointer[0][tag] = '<START>'\n",
    "\n",
    "    # Recursion\n",
    "    for t in range(1, n):\n",
    "        for tag in tags:\n",
    "            max_prob, prev_tag = max(\n",
    "                (viterbi_table[t-1][prev_tag] * transition_probs[prev_tag].get(tag, 0) * emission_probs[tag].get(sentence[t], 0), prev_tag)\n",
    "                for prev_tag in tags\n",
    "            )\n",
    "            viterbi_table[t][tag] = max_prob\n",
    "            backpointer[t][tag] = prev_tag\n",
    "    \n",
    "    # Termination\n",
    "    max_prob, last_tag = max(\n",
    "        (viterbi_table[n-1][tag] * transition_probs[tag].get('<STOP>', 0), tag)\n",
    "        for tag in tags\n",
    "    )\n",
    "    \n",
    "    # Backtrace\n",
    "    best_path = []\n",
    "    current_tag = last_tag\n",
    "    for t in range(n-1, -1, -1):\n",
    "        best_path.insert(0, current_tag)\n",
    "        current_tag = backpointer[t][current_tag]\n",
    "    \n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading treebank data...\n",
      "Train set size:  51681\n",
      "Dev set size:  7863\n",
      "Test set size:  9046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Stell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           #       1.00      0.27      0.43        22\n",
      "           $       1.00      0.54      0.70      1138\n",
      "          ''       1.00      0.65      0.79      1423\n",
      "           (       1.00      0.51      0.68       249\n",
      "           )       1.00      0.52      0.69       252\n",
      "           ,       1.00      0.54      0.70      9056\n",
      "           .       1.00      0.64      0.78      7035\n",
      "           :       1.00      0.55      0.71       983\n",
      "          CC       1.00      0.57      0.72      4289\n",
      "          CD       1.00      0.56      0.71      6023\n",
      "          DT       0.99      0.60      0.75     14946\n",
      "          EX       0.97      0.74      0.84       174\n",
      "          FW       0.70      0.18      0.29        38\n",
      "          IN       0.98      0.57      0.72     18147\n",
      "       IN|RB       0.00      0.00      0.00         0\n",
      "          JJ       0.90      0.53      0.67     10704\n",
      "         JJR       0.83      0.58      0.68       581\n",
      "     JJR|RBR       0.00      0.00      0.00         4\n",
      "         JJS       0.94      0.59      0.73       374\n",
      "       JJ|IN       0.00      0.00      0.00         1\n",
      "          LS       1.00      0.73      0.85        15\n",
      "          MD       1.00      0.65      0.79      1674\n",
      "          NN       0.97      0.58      0.73     23468\n",
      "         NNP       0.97      0.49      0.65     17236\n",
      "        NNPS       0.21      0.22      0.22       239\n",
      "    NNPS|NNS       0.00      0.00      0.00         0\n",
      "         NNS       0.99      0.59      0.74     10697\n",
      "         PDT       0.55      0.50      0.52        66\n",
      "         POS       0.98      0.56      0.71      1638\n",
      "         PRP       1.00      0.67      0.80      2930\n",
      "        PRP$       0.99      0.57      0.72      1433\n",
      "          RB       0.90      0.58      0.70      5853\n",
      "         RBR       0.79      0.48      0.60       382\n",
      "     RBR|JJR       0.00      0.00      0.00         2\n",
      "         RBS       0.79      0.52      0.62        87\n",
      "       RB|JJ       0.00      0.00      0.00         2\n",
      "          RP       0.43      0.47      0.45       356\n",
      "         SYM       1.00      0.91      0.95        11\n",
      "          TO       1.00      0.61      0.76      3899\n",
      "          UH       0.64      0.35      0.45        20\n",
      "          VB       0.94      0.60      0.73      4766\n",
      "         VBD       0.96      0.57      0.72      5869\n",
      "         VBG       0.94      0.53      0.67      2592\n",
      "      VBG|NN       0.00      0.00      0.00         1\n",
      "         VBN       0.86      0.53      0.65      3580\n",
      "      VBN|JJ       0.00      0.00      0.00         2\n",
      "         VBP       0.93      0.60      0.73      2209\n",
      "         VBZ       0.97      0.58      0.72      3608\n",
      "         WDT       0.87      0.64      0.74       773\n",
      "          WP       0.99      0.68      0.80       397\n",
      "         WP$       1.00      0.53      0.69        47\n",
      "         WRB       1.00      0.66      0.80       425\n",
      "          ``       0.02      1.00      0.04      1422\n",
      "\n",
      "    accuracy                           0.57    171138\n",
      "   macro avg       0.75      0.48      0.57    171138\n",
      "weighted avg       0.96      0.57      0.71    171138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Set path for datadir\n",
    "    datadir = \"data\\penn-treeban3-wsj\\wsj\"\n",
    "\n",
    "    train, dev, test = load_treebank_splits(datadir)\n",
    "\n",
    "    # Compute transition and emission probabilities\n",
    "    transition_probs, emission_probs = compute_transition_emission_tables(train)\n",
    "    tags = list(transition_probs.keys())\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_sentences = [get_token_tag_tuples(sent) for sent in test]\n",
    "    tagged_test_sentences = []\n",
    "    for sentence in test_sentences:\n",
    "        tokens = [token for token, _ in sentence]\n",
    "        predicted_tags = viterbi(tokens, transition_probs, emission_probs, tags)\n",
    "        tagged_test_sentences.append(list(zip(tokens, predicted_tags)))\n",
    "    \n",
    "    evaluate(test_sentences, tagged_test_sentences)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
